diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 509398d..8d59083 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1 +1,13 @@
 Auto generated patch. Do not edit or delete it, even if empty.
+diff -ruN --strip-trailing-cr a/clang/test/CIR/emit-actions.cpp b/clang/test/CIR/emit-actions.cpp
+--- a/clang/test/CIR/emit-actions.cpp
++++ b/clang/test/CIR/emit-actions.cpp
+@@ -11,7 +11,7 @@
+ 
+ int x = 1;
+ 
+-// BC: @x = dso_local global i32 1
++// BC: @x = {{(dso_local )?}}global i32 1
+ 
+ // ASM: x:
+ // ASM: .long   1
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index aa882fa..3bd54d0 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "760ec2c38e0cd01c016c403301e8dc081e0fc85c"
-    LLVM_SHA256 = "4ff64ffd55944e4ff6eda29c3c0191b94c64e3c2792d6e73af724db85b711be6"
+    LLVM_COMMIT = "dca73063653ca7d35afb3226ae66623495086204"
+    LLVM_SHA256 = "e9688256f2ace33836fb10f47481c1a934f632a9527f4f8d5b28fbd011528a75"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index a879e80..ed7a4af 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -748,290 +748,4 @@ diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo_quantized.mlir b/stablehlo/
  // -----
  
  func.func @uniform_quantized_c1(%arg0: tensor<2xf32>) {
-diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_legalize_quant_to_int.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_legalize_quant_to_int.mlir
---- stablehlo/stablehlo/tests/transforms/stablehlo_legalize_quant_to_int.mlir
-+++ stablehlo/stablehlo/tests/transforms/stablehlo_legalize_quant_to_int.mlir
-@@ -2675,3 +2675,73 @@
-     -> tensor<128x26x26x128x!quant.uniform<i8:f32, 1.000000e+00:5>>
-   return %0 : tensor<128x26x26x128x!quant.uniform<i8:f32, 1.000000e+00:5>>
- }
-+
-+// -----
-+
-+// CHECK-LABEL: func.func public @conv2d_nchw
-+// CHECK-SAME: %[[VAL_0:.*]]: tensor<1x3x224x224xi8>,
-+// CHECK-SAME: %[[VAL_1:.*]]: tensor<64x3x7x7xi8>) -> tensor<1x64x112x112xi8> {
-+func.func public @conv2d_nchw(
-+  %arg0: tensor<1x3x224x224x!quant.uniform<i8:f32, 3.158280e-02:1>>,
-+  %arg1: tensor<64x3x7x7x!quant.uniform<i8:f32, 3.101380e-03>>
-+  ) -> tensor<1x64x112x112x!quant.uniform<i8:f32, 1.438440e-02:-128>> {
-+// CHECK: stablehlo.convolution
-+// CHECK-SAME: dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1]
-+// CHECK-SAME: window = {
-+// CHECK-SAME: stride = [2, 2]
-+// CHECK-SAME{LITERAL}: pad = [[0, 0], [0, 0]]
-+// CHECK-SAME: rhs_dilate = [1, 1]
-+// CHECK-SAME: }
-+// CHECK-SAME: {
-+// CHECK-SAME: batch_group_count = 1 : i64
-+// CHECK-SAME: feature_group_count = 1 : i64
-+// CHECK-SAME: }
-+// CHECK-SAME : (tensor<1x3x230x230xi8>, tensor<64x3x7x7xi8>) -> tensor<1x64x112x112xi32>
-+  %0 = stablehlo.convolution(%arg0, %arg1)
-+    dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1],
-+    window = {
-+      stride = [2, 2],
-+      pad = [[3, 3], [3, 3]],
-+      rhs_dilate = [1, 1]
-+    } {
-+      batch_group_count = 1 : i64,
-+      feature_group_count = 1 : i64
-+    } : (
-+      tensor<1x3x224x224x!quant.uniform<i8:f32, 3.158280e-02:1>>,
-+      tensor<64x3x7x7x!quant.uniform<i8:f32, 3.101380e-03>>
-+  ) -> tensor<1x64x112x112x!quant.uniform<i8:f32, 1.438440e-02:-128>>
-+  return %0 : tensor<1x64x112x112x!quant.uniform<i8:f32, 1.438440e-02:-128>>
-+}
-+
-+// -----
-+
-+// CHECK-LABEL: func.func @conv3d_ncdhw
-+func.func @conv3d_ncdhw(
-+    %arg0: tensor<128x1x28x28x28x!quant.uniform<i8:f32, 2.000000e+00:4>>,
-+    %arg1: tensor<128x1x3x3x3x!quant.uniform<i8:f32, 3.000000e+00:0>>) {
-+// CHECK: stablehlo.convolution
-+// CHECK-SAME: dim_numbers = [b, f, 0, 1, 2]x[o, i, 0, 1, 2]->[b, f, 0, 1, 2],
-+// CHECK-SAME: window = {
-+// CHECK-SAME{LITERAL}: stride = [1, 1, 1], pad = [[0, 0], [0, 0], [0, 0]],
-+// CHECK-SAME: lhs_dilate = [1, 1, 1],
-+// CHECK-SAME: rhs_dilate = [1, 1, 1]
-+// CHECK-SAME: }
-+// CHECK-SAME: {
-+// CHECK-SAME: batch_group_count = 1 : i64,
-+// CHECK-SAME: feature_group_count = 1 : i64
-+// CHECK-SAME: }
-+// CHECK-SAME: : (tensor<128x1x28x28x28xf32>, tensor<128x1x3x3x3xf32>) -> tensor<128x128x26x26x26xf32>
-+  %0 = stablehlo.convolution(%arg0, %arg1)
-+    dim_numbers = [b, f, 0, 1, 2]x[o, i, 0, 1, 2]->[b, f, 0, 1, 2],
-+    window = {
-+      stride = [1, 1, 1], pad = [[0, 0], [0, 0], [0, 0]],
-+      lhs_dilate = [1, 1, 1],
-+      rhs_dilate = [1, 1, 1]
-+    }
-+    {
-+      batch_group_count = 1 : i64,
-+      feature_group_count = 1 : i64
-+    } : (tensor<128x1x28x28x28x!quant.uniform<i8:f32, 2.000000e+00:4>>, tensor<128x1x3x3x3x!quant.uniform<i8:f32, 3.000000e+00:0>>)
-+    -> tensor<128x128x26x26x26x!quant.uniform<i32:f32, 1.000000e+00:5>>
-+  return
-+}
-diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeQuantToMath.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeQuantToMath.cpp
---- stablehlo/stablehlo/transforms/StablehloLegalizeQuantToMath.cpp
-+++ stablehlo/stablehlo/transforms/StablehloLegalizeQuantToMath.cpp
-@@ -766,6 +766,7 @@
-   Value outputDimsValue = nullptr;
-   // Calculate LHS contribution when RHS zp is non-zero.
-   if (rhsZp != 0) {
-+    // Contributions from RHS_ZP * LHS.
-     SmallVector<int64_t> reductionDims = to_vector(llvm::concat<const int64_t>(
-         dims.lhsSpatialDims, dims.lhsContractingDims));
-     Value lhsZpContribution =
-@@ -778,15 +779,23 @@
-   }
-   // Calculate RHS contribution when LHS zp is non-zero.
-   if (lhsZp != 0) {
-+    // Contributions from LHS_ZP * RHS.
-     SmallVector<int64_t> reductionDims = to_vector(llvm::concat<const int64_t>(
-         dims.rhsSpatialDims, dims.rhsContractingDims));
-     Value rhsZpContribution =
-         createZeroPointPartialOffset(builder, loc, rhs, lhsZp, reductionDims);
-+
-+    int64_t nonBatchingStartingIdx =
-+        lhsShape.getRank() - dims.lhsContractingDims.size();
-+    if (!dims.lhsSpatialDims.empty() && !dims.rhsSpatialDims.empty()) {
-+      // In case of convolution, identified via absence of spatial dims, the
-+      // non-batching starting index is the lhs contracting dim.
-+      nonBatchingStartingIdx = dims.lhsContractingDims[0];
-+    }
-     // Broadcast rhs ZP contribution to result tensor shape.
-     rhsZpContribution = broadcastZpContribution(
-         builder, loc, rhsZpContribution, reductionDims, dims.rhsBatchingDims,
--        lhsShape.getRank() - dims.lhsContractingDims.size(), output,
--        outputTensorType, outputDimsValue);
-+        nonBatchingStartingIdx, output, outputTensorType, outputDimsValue);
-     if (result) {
-       result = builder.create<stablehlo::AddOp>(loc, result, rhsZpContribution);
-     } else {
-@@ -833,7 +842,8 @@
- template <typename DotLikeOp>
- Value createDotLikeKernel(OpBuilder &builder, Location loc, DotLikeOp,
-                           Type resultType, Value &lhs, Value &rhs,
--                          ArrayRef<NamedAttribute> attrs) {
-+                          ArrayRef<NamedAttribute> attrs,
-+                          const DotLikeDimensionNumbers &dims) {
-   return builder.create<stablehlo::DotGeneralOp>(
-       loc, resultType, ArrayRef<Value>{lhs, rhs}, attrs);
- }
-@@ -843,7 +853,8 @@
- template <>
- Value createDotLikeKernel<stablehlo::ConvolutionOp>(
-     OpBuilder &builder, Location loc, stablehlo::ConvolutionOp op,
--    Type resultType, Value &lhs, Value &rhs, ArrayRef<NamedAttribute> attrs) {
-+    Type resultType, Value &lhs, Value &rhs, ArrayRef<NamedAttribute> attrs,
-+    const DotLikeDimensionNumbers &dims) {
-   // We only handle the case where RHS zp is zero.
-   // Explicitly pad LHS with zp and update LHS value.
-   SmallVector<NamedAttribute> newAttrs(attrs);
-@@ -865,9 +876,9 @@
-     int64_t rank = cast<TensorType>(lhs.getType()).getRank();
-     SmallVector<int64_t> paddingLow(rank, 0), paddingHigh(rank, 0),
-         paddingInterior(rank, 0);
--    for (int64_t i = 1; i < rank - 1; ++i) {
--      paddingLow[i] = originalPadding[i * 2 - 2];
--      paddingHigh[i] = originalPadding[i * 2 - 1];
-+    for (int64_t i = 0; i < dims.lhsSpatialDims.size(); ++i) {
-+      paddingLow[dims.lhsSpatialDims[i]] = originalPadding[i * 2];
-+      paddingHigh[dims.lhsSpatialDims[i]] = originalPadding[i * 2 + 1];
-     }
-     lhs = builder.create<stablehlo::PadOp>(loc, lhs, zp, paddingLow,
-                                            paddingHigh, paddingInterior);
-@@ -897,6 +908,8 @@
-   Value rhs = adaptor.getRhs();
-   auto resInt32TensorType =
-       op.getResult().getType().clone(rewriter.getI32Type());
-+  auto resFloat32TensorType =
-+      op.getResult().getType().clone(rewriter.getF32Type());
- 
-   // Dot result
-   //   = dot((lhs - zp_l) * scale_l, (rhs - zp_r) * scale_r) / scale_res
-@@ -908,7 +921,7 @@
-   //   combined_zp = res_zp - zp_offset * combined_scale
-   //   zp_offset = zp_l*rhs + zp_r*lhs - zp_l*zp_r
-   Value resI32 = createDotLikeKernel(rewriter, op->getLoc(), op,
--                                     resInt32TensorType, lhs, rhs, attrs);
-+                                     resInt32TensorType, lhs, rhs, attrs, dims);
- 
-   auto lhsElementQuantType = getPerTensorType(op.getLhs().getType());
-   auto rhsElementQuantType = dyn_cast<quant::UniformQuantizedType>(
-@@ -945,8 +958,6 @@
-     Value combinedScale = rewriter.create<stablehlo::ConstantOp>(
-         op->getLoc(), rewriter.getF32FloatAttr(combinedScaleFp));
- 
--    auto resFloat32TensorType =
--        op.getResult().getType().clone(rewriter.getF32Type());
-     Value resF32 = rewriter.create<stablehlo::ConvertOp>(
-         op->getLoc(), resFloat32TensorType, resI32);
-     resF32 = rewriter.create<chlo::BroadcastMulOp>(
-@@ -1089,6 +1100,7 @@
- };
- 
- bool isConvNhwc(const stablehlo::ConvDimensionNumbersAttr &dims) {
-+  // lhs(b, 0, 1, f) x rhs(0, 1, i, o) -> res(b, 0, 1, f)
-   return dims.getInputBatchDimension() == 0 &&
-          dims.getInputFeatureDimension() == 3 &&
-          dims.getInputSpatialDimensions().size() == 2 &&
-@@ -1106,7 +1118,27 @@
-          dims.getOutputSpatialDimensions()[1] == 2;
- }
- 
-+bool isConvNchw(const stablehlo::ConvDimensionNumbersAttr &dims) {
-+  // lhs(b, f, 0, 1) x rhs(o, i, 0, 1) -> res(b, f, 0, 1)
-+  return dims.getInputBatchDimension() == 0 &&
-+         dims.getInputFeatureDimension() == 1 &&
-+         dims.getInputSpatialDimensions().size() == 2 &&
-+         dims.getInputSpatialDimensions()[0] == 2 &&
-+         dims.getInputSpatialDimensions()[1] == 3 &&
-+         dims.getKernelInputFeatureDimension() == 1 &&
-+         dims.getKernelOutputFeatureDimension() == 0 &&
-+         dims.getKernelSpatialDimensions().size() == 2 &&
-+         dims.getKernelSpatialDimensions()[0] == 2 &&
-+         dims.getKernelSpatialDimensions()[1] == 3 &&
-+         dims.getOutputBatchDimension() == 0 &&
-+         dims.getOutputFeatureDimension() == 1 &&
-+         dims.getOutputSpatialDimensions().size() == 2 &&
-+         dims.getOutputSpatialDimensions()[0] == 2 &&
-+         dims.getOutputSpatialDimensions()[1] == 3;
-+}
-+
- bool isConvNDHWC(const stablehlo::ConvDimensionNumbersAttr &dims) {
-+  // lhs(b, 0, 1, 2, f) x rhs(0, 1, 2, i, o) -> res(b, 0, 1, 2, f)
-   return dims.getInputBatchDimension() == 0 &&
-          dims.getInputFeatureDimension() == 4 &&
-          dims.getInputSpatialDimensions().size() == 3 &&
-@@ -1127,6 +1159,28 @@
-          dims.getOutputSpatialDimensions()[2] == 3;
- }
- 
-+bool isConvNCDHW(const stablehlo::ConvDimensionNumbersAttr &dims) {
-+  // lhs(b, f, 0, 1, 2) x rhs(o, i, 0, 1, 2) -> res(b, f, 0, 1, 2)
-+  return dims.getInputBatchDimension() == 0 &&
-+         dims.getInputFeatureDimension() == 1 &&
-+         dims.getInputSpatialDimensions().size() == 3 &&
-+         dims.getInputSpatialDimensions()[0] == 2 &&
-+         dims.getInputSpatialDimensions()[1] == 3 &&
-+         dims.getInputSpatialDimensions()[2] == 4 &&
-+         dims.getKernelInputFeatureDimension() == 0 &&
-+         dims.getKernelOutputFeatureDimension() == 1 &&
-+         dims.getKernelSpatialDimensions().size() == 3 &&
-+         dims.getKernelSpatialDimensions()[0] == 2 &&
-+         dims.getKernelSpatialDimensions()[1] == 3 &&
-+         dims.getKernelSpatialDimensions()[2] == 4 &&
-+         dims.getOutputBatchDimension() == 0 &&
-+         dims.getOutputFeatureDimension() == 1 &&
-+         dims.getOutputSpatialDimensions().size() == 3 &&
-+         dims.getOutputSpatialDimensions()[0] == 2 &&
-+         dims.getOutputSpatialDimensions()[1] == 3 &&
-+         dims.getOutputSpatialDimensions()[2] == 4;
-+}
-+
- FailureOr<DotLikeDimensionNumbers> verifyAndConstructDims(
-     stablehlo::ConvolutionOp op, ConversionPatternRewriter &rewriter) {
-   // RHS (weight) must have zero zp.
-@@ -1185,7 +1239,8 @@
-   // We only support NHWC Conv2D and NDHWC Conv3D.
-   auto dims = op.getDimensionNumbers();
-   if (isConvNhwc(dims)) {
--    // 2D Convolution.
-+    // 2D Nhwc Convolution.
-+    // lhs(b, 0, 1, f) x rhs(0, 1, i, o) -> res(b, 0, 1, f)
-     return DotLikeDimensionNumbers{/*lhs_batching_dims=*/{0},
-                                    /*lhs_spatial_dims=*/{1, 2},
-                                    /*lhs_contracting_dims=*/{3},
-@@ -1193,14 +1248,35 @@
-                                    /*rhs_spatial_dims=*/{0, 1},
-                                    /*rhs_contracting_dims=*/{2}};
-   }
-+  if (isConvNchw(dims)) {
-+    // 2D Nchw Convolution.
-+    // lhs(b, f, 0, 1) x rhs(o, i, 0, 1) -> res(b, f, 0, 1)
-+    return DotLikeDimensionNumbers{/*lhs_batching_dims=*/{0},
-+                                   /*lhs_spatial_dims=*/{2, 3},
-+                                   /*lhs_contracting_dims=*/{1},
-+                                   /*rhs_batching_dims=*/{},
-+                                   /*rhs_spatial_dims=*/{2, 3},
-+                                   /*rhs_contracting_dims=*/{1}};
-+  }
-   if (isConvNDHWC(dims)) {
--    // 3D Convolution.
-+    // 3D Ndhwc Convolution.
-+    // lhs(b, 0, 1, 2, f) x rhs(0, 1, 2, i, o) -> res(b, 0, 1, 2, f)
-     return DotLikeDimensionNumbers{/*lhs_batching_dims=*/{0},
-                                    /*lhs_spatial_dims=*/{1, 2, 3},
-                                    /*lhs_contracting_dims=*/{4},
-                                    /*rhs_batching_dims=*/{},
-                                    /*rhs_spatial_dims=*/{0, 1, 2},
-                                    /*rhs_contracting_dims=*/{3}};
-+  }
-+  if (isConvNCDHW(dims)) {
-+    // 3D Ncdhw Convolution.
-+    // lhs(b, f, 0, 1, 2) x rhs(o, i, 0, 1, 2) -> res(b, f, 0, 1, 2)
-+    return DotLikeDimensionNumbers{/*lhs_batching_dims=*/{0},
-+                                   /*lhs_spatial_dims=*/{2, 3, 4},
-+                                   /*lhs_contracting_dims=*/{1},
-+                                   /*rhs_batching_dims=*/{},
-+                                   /*rhs_spatial_dims=*/{2, 3, 4},
-+                                   /*rhs_contracting_dims=*/{1}};
-   }
-   return rewriter.notifyMatchFailure(op,
-                                      "Convolution data format must be NHWC.");
 
